{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88221d1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scans/FLD_1_cut.mhd loaded (100, 100, 100) \n",
      "\n",
      "Scans/FLD_2_cut.mhd loaded (100, 100, 100) \n",
      "\n",
      "Scans/FLD_3_cut.mhd loaded (100, 100, 100) \n",
      "\n",
      "Scans/FLD_10_cut.mhd loaded (100, 100, 100) \n",
      "\n",
      "Scans/FLD_11_cut.mhd loaded (100, 100, 100) \n",
      "\n",
      "Scans/FLD_12_cut.mhd loaded (100, 100, 100) \n",
      "\n",
      "Scans/TGA_1_cut.mhd loaded (100, 100, 100) \n",
      "\n",
      "Scans/TGA_4_cut.mhd loaded (100, 100, 100) \n",
      "\n",
      "Scans/041022_Blarr_CF_20-47_FLD_C-F_3_F_1-3-0123,00_recon_F1_BC.mhd loaded (100, 100, 100) \n",
      "\n",
      "Scans/041022_Blarr_CF_20-47_FLD_C-F_3_F_1-3-0123,00_recon_F2_BC.mhd loaded (100, 100, 100) \n",
      "\n",
      "Scans/041022_Blarr_CF_20-47_FLD_C-F_3_F_1-3-0123,00_recon_F3_BC.mhd loaded (100, 100, 100) \n",
      "\n",
      "Scans/041022_Blarr_CF_20-47_FLD_C-F_3_F_1-3-0123,00_recon_CF1_BC.mhd loaded (100, 100, 100) \n",
      "\n",
      "Scans/041022_Blarr_CF_20-47_FLD_C-F_3_F_1-3-0123,00_recon_CF2_BC.mhd loaded (100, 100, 100) \n",
      "\n",
      "Scans/041022_Blarr_CF_20-47_FLD_C-F_3_F_1-3-0123,00_recon_CF3_BC.mhd loaded (100, 100, 100) \n",
      "\n",
      "Scans/041022_Blarr_CF_20-47_FLD_C-F_3_F_1-3-0123,00_recon_C1_BC.mhd loaded (100, 100, 100) \n",
      "\n",
      "Scans/041022_Blarr_CF_20-47_FLD_C-F_3_F_1-3-0123,00_recon_C2_BC.mhd loaded (100, 100, 100) \n",
      "\n",
      "Scans/041022_Blarr_CF_20-47_FLD_C-F_3_F_1-3-0123,00_recon_C3_BC.mhd loaded (100, 100, 100) \n",
      "\n",
      "CPU times: user 1min 47s, sys: 9.33 s, total: 1min 57s\n",
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "source": [
    "# Importing Packages\n",
    "import os\n",
    "from PIL import Image\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from skimage import transform\n",
    "from sys import getsizeof\n",
    "from functions_cnn import *\n",
    "\n",
    "# Loading Images\n",
    "newImageResolution = 100\n",
    "FLD1  = makeCubeArray('Scans/FLD_1_cut.mhd', newImageResolution, adjustScans=True)\n",
    "FLD2  = makeCubeArray('Scans/FLD_2_cut.mhd', newImageResolution, adjustScans=True)\n",
    "FLD3  = makeCubeArray('Scans/FLD_3_cut.mhd', newImageResolution, adjustScans=True)\n",
    "FLD10 = makeCubeArray('Scans/FLD_10_cut.mhd', newImageResolution, adjustScans=True)\n",
    "FLD11 = makeCubeArray('Scans/FLD_11_cut.mhd', newImageResolution, adjustScans=True)\n",
    "FLD12 = makeCubeArray('Scans/FLD_12_cut.mhd', newImageResolution, adjustScans=True)\n",
    "F1 = makeCubeArray('Scans/041022_Blarr_CF_20-47_FLD_C-F_3_F_1-3-0123,00_recon_F1_BC.mhd', newImageResolution, adjustScans=True)\n",
    "F2 = makeCubeArray('Scans/041022_Blarr_CF_20-47_FLD_C-F_3_F_1-3-0123,00_recon_F2_BC.mhd', newImageResolution, adjustScans=True)\n",
    "F3 = makeCubeArray('Scans/041022_Blarr_CF_20-47_FLD_C-F_3_F_1-3-0123,00_recon_F3_BC.mhd', newImageResolution, adjustScans=True)\n",
    "CF1 = makeCubeArray('Scans/041022_Blarr_CF_20-47_FLD_C-F_3_F_1-3-0123,00_recon_CF1_BC.mhd', newImageResolution, adjustScans=True)\n",
    "CF2 = makeCubeArray('Scans/041022_Blarr_CF_20-47_FLD_C-F_3_F_1-3-0123,00_recon_CF2_BC.mhd', newImageResolution, adjustScans=True)\n",
    "CF3 = makeCubeArray('Scans/041022_Blarr_CF_20-47_FLD_C-F_3_F_1-3-0123,00_recon_CF3_BC.mhd', newImageResolution, adjustScans=True)\n",
    "C1 = makeCubeArray('Scans/041022_Blarr_CF_20-47_FLD_C-F_3_F_1-3-0123,00_recon_C1_BC.mhd', newImageResolution, adjustScans=True)\n",
    "C2 = makeCubeArray('Scans/041022_Blarr_CF_20-47_FLD_C-F_3_F_1-3-0123,00_recon_C2_BC.mhd', newImageResolution, adjustScans=True)\n",
    "C3 = makeCubeArray('Scans/041022_Blarr_CF_20-47_FLD_C-F_3_F_1-3-0123,00_recon_C3_BC.mhd', newImageResolution, adjustScans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1aca953",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 Bit array size:  112000.16  KB\n",
      "16 Bit array size:  28000.16  KB \n",
      "\n",
      "Data Shape before Augemntation: (14, 100, 100, 100)\n",
      "(28, 100, 100, 100)\n",
      "(42, 100, 100, 100)\n",
      "(56, 100, 100, 100)\n",
      "(112, 100, 100, 100)\n",
      "(224, 100, 100, 100)\n",
      "(448, 100, 100, 100)\n",
      "Data Shape after Augemntation: (448, 100, 100, 100) \n",
      "\n",
      "The FibrePerc list was multiplied 32.0 times \n",
      "\n",
      "split:  299 / 149\n"
     ]
    }
   ],
   "source": [
    "# Gathering Images in a dictionary\n",
    "# The values represent the FVCs that were determined experimentally\n",
    "Data = {'FLD1':0.223, \n",
    "        'FLD2':0.255, \n",
    "        'FLD3':0.286, \n",
    "        #'FLD10':0.179, #This scan caused unexplained abnormalities and is thus removed\n",
    "        'FLD11':0.240, \n",
    "        'FLD12':0.266,\n",
    "        'F1':0.230669,\n",
    "        'F2':0.220811,\n",
    "        'F3':0.230589,\n",
    "        'CF1':0.255740,\n",
    "        'CF2':0.223137,\n",
    "        'CF3':0.228111,\n",
    "        'C1':0.263567,\n",
    "        'C2':0.231027,\n",
    "        'C3':0.238066,\n",
    "}\n",
    "\n",
    "keys = [*Data.keys()] #Contains all Fibre Percentages\n",
    "n0 = len(keys) #Initial amount of scans used\n",
    "\n",
    "# Making array containing the 3D-arrays of all scans\n",
    "Scans_float64 = np.array([globals()[keys[i]] for i in range(n0)])\n",
    "print(\"64 Bit array size: \", getsizeof(Scans_float64)/1000, \" KB\")\n",
    "\n",
    "# Reducing all values to 16 bits (equivalent to the maximum original information content of each CT scan)\n",
    "Scans_float16 = np.array([np.float16(Scans_float64[i]) for i in range(n0)])\n",
    "print(\"16 Bit array size: \", getsizeof(Scans_float16)/1000, \" KB\", \"\\n\")\n",
    "Scans = Scans_float16 #For simplicity, the variable name is adjusted\n",
    "\n",
    "# Augmentation\n",
    "print(\"Data Shape before Augemntation:\", Scans.shape)\n",
    "#Defining vectors for the geometrical operations\n",
    "direct_x = 0\n",
    "direct_y = 1\n",
    "direct_z = 2\n",
    "normalPlane_x = (1,2)\n",
    "normalPlane_y = (2,0)\n",
    "normalPlane_z = (0,1)\n",
    "#Expanding the Scan set with augmented versions\n",
    "#Both size inputs are identical to create cuboid-shaped arrays to allow for more augmentation steps \n",
    "Scans = addRotations(Scans, normalPlane_x, n0, newImageResolution, newImageResolution) #Adding 90° x-rotated scans to \"Scans\" array\n",
    "Scans = addRotations(Scans, normalPlane_y, n0, newImageResolution, newImageResolution) #Adding 90° y-rotated scans to \"Scans\" array\n",
    "Scans = addRotations(Scans, normalPlane_z, n0, newImageResolution, newImageResolution) #Adding 90° z-rotated scans to \"Scans\" array\n",
    "Scans = addFlips(Scans, direct_x, newImageResolution, newImageResolution) #Adding x-flipped scants to \"Scans\" array\n",
    "Scans = addFlips(Scans, direct_y, newImageResolution, newImageResolution) #Adding y-flipped scants to \"Scans\" array\n",
    "Scans = addFlips(Scans, direct_z, newImageResolution, newImageResolution) #Adding z-flipped scants to \"Scans\" array\n",
    "print(\"Data Shape after Augemntation:\", Scans.shape, \"\\n\")\n",
    "\n",
    "# Creating the fiber precentages list\n",
    "FibrePerc = np.array([*Data.values()])\n",
    "fvc_appended = FibrePerc\n",
    "for i in range(int(Scans.shape[0] / n0) - 1): # Mulitplying the Fibre Precentage list\n",
    "    fvc_appended = np.append(fvc_appended, FibrePerc)\n",
    "FibrePerc = fvc_appended\n",
    "print(\"The FibrePerc list was multiplied\", Scans.shape[0] / n0, \"times\", \"\\n\")\n",
    "\n",
    "# Defining the Split between input and validation Data\n",
    "split_border = 2/3\n",
    "split = round(len(Scans) * split_border) #Using 1/3 of Data for validation\n",
    "print(\"split: \", split, \"/\", len(Scans)-split)\n",
    "\n",
    "inputScans = Scans[:split] #Split for training data\n",
    "InputScans = tf.expand_dims(inputScans, axis = 4)\n",
    "inputLabels = FibrePerc[:split]\n",
    "\n",
    "testScans = Scans[split:] #Split for validation data\n",
    "TestScans = tf.expand_dims(testScans, axis = 4)\n",
    "testLabels = FibrePerc[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7e33bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final CNN\n",
    "\n",
    "def get_model(depth, width, height):\n",
    "    inputs = keras.Input((depth, width, height, 1))\n",
    "    \n",
    "    x = tf.keras.layers.Normalization(axis=-1, invert=True)\n",
    "\n",
    "    x = layers.Conv3D(filters=2, kernel_size=3, activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPooling3D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.Dense(units=64, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Dense(units=128, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling3D()(x)\n",
    "    \n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
    "    return keras.Model(inputs, outputs, name=\"3dcnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97fc0ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"3dcnn\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 100, 100, 100, 1  0         \n",
      "                             )]                                  \n",
      "                                                                 \n",
      " conv3d_5 (Conv3D)           (None, 98, 98, 98, 2)     56        \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPooling  (None, 49, 49, 49, 2)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 49, 49, 49, 64)    192       \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 49, 49, 49, 64)    0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 49, 49, 49, 128)   8320      \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 49, 49, 49, 128)   0         \n",
      "                                                                 \n",
      " global_average_pooling3d_5   (None, 128)              0         \n",
      " (GlobalAveragePooling3D)                                        \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,697\n",
      "Trainable params: 8,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10/10 [==============================] - 20s 2s/step\n",
      "SAD: 0.2586218689920687\n",
      "SAD relative: 1.07894398893053\n",
      "Epoch 1/40\n",
      "10/10 [==============================] - 104s 10s/step - loss: 0.6928 - mae: 0.2573 - val_loss: 0.6905 - val_mae: 0.2554\n",
      "Epoch 2/40\n",
      "10/10 [==============================] - 104s 10s/step - loss: 0.6891 - mae: 0.2537 - val_loss: 0.6869 - val_mae: 0.2518\n",
      "Epoch 3/40\n",
      "10/10 [==============================] - 105s 10s/step - loss: 0.6856 - mae: 0.2501 - val_loss: 0.6832 - val_mae: 0.2482\n",
      "Epoch 4/40\n",
      "10/10 [==============================] - 105s 11s/step - loss: 0.6816 - mae: 0.2462 - val_loss: 0.6790 - val_mae: 0.2439\n",
      "Epoch 5/40\n",
      "10/10 [==============================] - 106s 11s/step - loss: 0.6771 - mae: 0.2416 - val_loss: 0.6742 - val_mae: 0.2389\n",
      "Epoch 6/40\n",
      "10/10 [==============================] - 106s 11s/step - loss: 0.6720 - mae: 0.2363 - val_loss: 0.6686 - val_mae: 0.2330\n",
      "Epoch 7/40\n",
      "10/10 [==============================] - 110s 11s/step - loss: 0.6662 - mae: 0.2301 - val_loss: 0.6623 - val_mae: 0.2262\n",
      "Epoch 8/40\n",
      "10/10 [==============================] - 111s 11s/step - loss: 0.6597 - mae: 0.2229 - val_loss: 0.6552 - val_mae: 0.2183\n",
      "Epoch 9/40\n",
      "10/10 [==============================] - 110s 11s/step - loss: 0.6525 - mae: 0.2147 - val_loss: 0.6474 - val_mae: 0.2093\n",
      "Epoch 10/40\n",
      "10/10 [==============================] - 110s 11s/step - loss: 0.6446 - mae: 0.2054 - val_loss: 0.6389 - val_mae: 0.1990\n",
      "Epoch 11/40\n",
      "10/10 [==============================] - 112s 11s/step - loss: 0.6360 - mae: 0.1950 - val_loss: 0.6298 - val_mae: 0.1875\n",
      "Epoch 12/40\n",
      "10/10 [==============================] - 110s 11s/step - loss: 0.6270 - mae: 0.1833 - val_loss: 0.6204 - val_mae: 0.1748\n",
      "Epoch 13/40\n",
      "10/10 [==============================] - 111s 11s/step - loss: 0.6176 - mae: 0.1704 - val_loss: 0.6107 - val_mae: 0.1609\n",
      "Epoch 14/40\n",
      "10/10 [==============================] - 111s 11s/step - loss: 0.6082 - mae: 0.1565 - val_loss: 0.6011 - val_mae: 0.1460\n",
      "Epoch 15/40\n",
      "10/10 [==============================] - 112s 11s/step - loss: 0.5988 - mae: 0.1415 - val_loss: 0.5919 - val_mae: 0.1302\n",
      "Epoch 16/40\n",
      "10/10 [==============================] - 111s 11s/step - loss: 0.5899 - mae: 0.1258 - val_loss: 0.5834 - val_mae: 0.1140\n",
      "Epoch 17/40\n",
      "10/10 [==============================] - 112s 11s/step - loss: 0.5817 - mae: 0.1098 - val_loss: 0.5757 - val_mae: 0.0976\n",
      "Epoch 18/40\n",
      "10/10 [==============================] - 112s 11s/step - loss: 0.5744 - mae: 0.0937 - val_loss: 0.5693 - val_mae: 0.0817\n",
      "Epoch 19/40\n",
      "10/10 [==============================] - 112s 11s/step - loss: 0.5684 - mae: 0.0780 - val_loss: 0.5641 - val_mae: 0.0664\n",
      "Epoch 20/40\n",
      "10/10 [==============================] - 113s 11s/step - loss: 0.5635 - mae: 0.0632 - val_loss: 0.5601 - val_mae: 0.0523\n",
      "Epoch 21/40\n",
      "10/10 [==============================] - 113s 11s/step - loss: 0.5599 - mae: 0.0497 - val_loss: 0.5573 - val_mae: 0.0399\n",
      "Epoch 22/40\n",
      "10/10 [==============================] - 108s 11s/step - loss: 0.5574 - mae: 0.0385 - val_loss: 0.5555 - val_mae: 0.0307\n",
      "Epoch 23/40\n",
      "10/10 [==============================] - 106s 11s/step - loss: 0.5558 - mae: 0.0299 - val_loss: 0.5545 - val_mae: 0.0239\n",
      "Epoch 24/40\n",
      "10/10 [==============================] - 106s 11s/step - loss: 0.5548 - mae: 0.0238 - val_loss: 0.5539 - val_mae: 0.0191\n",
      "Epoch 25/40\n",
      "10/10 [==============================] - 107s 11s/step - loss: 0.5544 - mae: 0.0195 - val_loss: 0.5537 - val_mae: 0.0165\n",
      "Epoch 26/40\n",
      "10/10 [==============================] - 108s 11s/step - loss: 0.5541 - mae: 0.0172 - val_loss: 0.5536 - val_mae: 0.0153\n",
      "Epoch 27/40\n",
      "10/10 [==============================] - 109s 11s/step - loss: 0.5540 - mae: 0.0159 - val_loss: 0.5536 - val_mae: 0.0148\n",
      "Epoch 28/40\n",
      "10/10 [==============================] - 115s 12s/step - loss: 0.5540 - mae: 0.0153 - val_loss: 0.5536 - val_mae: 0.0147\n",
      "Epoch 29/40\n",
      "10/10 [==============================] - 110s 11s/step - loss: 0.5540 - mae: 0.0152 - val_loss: 0.5536 - val_mae: 0.0147\n",
      "Epoch 30/40\n",
      "10/10 [==============================] - 130s 13s/step - loss: 0.5539 - mae: 0.0151 - val_loss: 0.5536 - val_mae: 0.0148\n",
      "Epoch 31/40\n",
      "10/10 [==============================] - 150s 15s/step - loss: 0.5539 - mae: 0.0151 - val_loss: 0.5536 - val_mae: 0.0148\n",
      "Epoch 32/40\n",
      "10/10 [==============================] - 106s 11s/step - loss: 0.5539 - mae: 0.0150 - val_loss: 0.5536 - val_mae: 0.0149\n",
      "Epoch 33/40\n",
      "10/10 [==============================] - 105s 11s/step - loss: 0.5539 - mae: 0.0150 - val_loss: 0.5536 - val_mae: 0.0149\n",
      "Epoch 34/40\n",
      "10/10 [==============================] - 107s 11s/step - loss: 0.5539 - mae: 0.0150 - val_loss: 0.5536 - val_mae: 0.0149\n",
      "Epoch 35/40\n",
      "10/10 [==============================] - 108s 11s/step - loss: 0.5539 - mae: 0.0149 - val_loss: 0.5536 - val_mae: 0.0150\n",
      "Epoch 36/40\n",
      "10/10 [==============================] - 110s 11s/step - loss: 0.5539 - mae: 0.0149 - val_loss: 0.5536 - val_mae: 0.0150\n",
      "Epoch 37/40\n",
      "10/10 [==============================] - 108s 11s/step - loss: 0.5539 - mae: 0.0149 - val_loss: 0.5536 - val_mae: 0.0150\n",
      "Epoch 38/40\n",
      "10/10 [==============================] - 217s 22s/step - loss: 0.5539 - mae: 0.0149 - val_loss: 0.5536 - val_mae: 0.0150\n",
      "Epoch 39/40\n",
      "10/10 [==============================] - 330s 35s/step - loss: 0.5539 - mae: 0.0149 - val_loss: 0.5536 - val_mae: 0.0149\n",
      "Epoch 40/40\n",
      "10/10 [==============================] - 184s 19s/step - loss: 0.5539 - mae: 0.0149 - val_loss: 0.5536 - val_mae: 0.0149\n",
      "10/10 [==============================] - 20s 2s/step\n",
      "SAD: 0.015196767499001926\n",
      "SAD relative: 0.06176944702561246\n",
      "5/5 [==============================] - 29s 5s/step\n",
      "Result from test data prediction:\n",
      "SAD: 0.015009645490595155\n",
      "SAD relative: 0.06095630381514746\n"
     ]
    }
   ],
   "source": [
    "#Training and prediction\n",
    "\n",
    "#Defining adjustable learning rate\n",
    "initial_learning_rate = 0.0001\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "model = get_model(depth=newImageResolution, width=newImageResolution, height=newImageResolution) #Selecting the network architecture\n",
    "model.summary() #prints out the overview over the network structure\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\"\n",
    "    ,optimizer=keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    ,metrics=['mae']\n",
    ")\n",
    "\n",
    "# Initial prediction (without training)\n",
    "y_m1 = model.predict(InputScans)\n",
    "y_m1 = np.around(y_m1, 3).flatten() #reshape data for better visualisaition\n",
    "deviation(inputLabels, y_m1)\n",
    "\n",
    "# Train model\n",
    "epochs = 40\n",
    "hist = model.fit(Scans, FibrePerc, epochs = epochs, validation_split = 1 - split_border, shuffle = True)\n",
    "\n",
    "# New prediction (after training)\n",
    "y_m2 = model.predict(InputScans)\n",
    "y_m2 = np.around(y_m2, 3).flatten() #reshape data for better visualisaition\n",
    "deviation(inputLabels, y_m2)\n",
    "\n",
    "# Predicting validation data\n",
    "y_m3 = model.predict(TestScans)\n",
    "y_m3 = np.around(y_m3, 3).flatten() #reshape data for better visualisaition\n",
    "print(\"Result from test data prediction:\")\n",
    "deviation(testLabels, y_m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96dd3911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport matplotlib\\nplt.rc(\\'text\\', usetex=True)\\nplt.rc(\\'font\\', family=\\'serif\\')\\nplt.rcParams.update({\\'font.size\\': 25})\\n\\nfig = plt.figure(figsize=(10, 6), dpi=250)\\nplt.plot(hist.history[\\'loss\\'])\\nplt.plot(hist.history[\\'val_loss\\'])\\n\\nplt.title(\\'model loss\\')\\nplt.ylabel(\\'loss\\')\\nplt.xlabel(\\'epoch\\')\\nplt.legend([\\'train\\', \\'val\\'], loc=\\'upper left\\')\\nplt.savefig(\\'loss_normal_network_without_normalization.eps\\', format=\\'eps\\')\\nplt.savefig(\\'loss_normal_network_without_normalization.png\\', dpi=250, bbox_inches=\"tight\")\\n#plt.show()\\n\\n#Plotting results for the validation Data\\nX_test = np.arange(len(Scans)-split)\\nfig = plt.figure(figsize=(10, 6), dpi=250)\\nax = fig.add_axes([0,0,1,1])\\nax.bar(X_test + 0.00, testLabels, color = \\'tab:red\\', width = 0.15)\\nax.bar(X_test + 0.25, y_m3, color = \\'green\\', width = 0.15)\\nax.set_ylabel(\\'Vol. fiber percentage\\')\\nax.set_xlabel(\\'testScans\\')\\nax.legend(labels=[\\'testLabels\\', \\'prediction\\'])\\nplt.savefig(\\'results_normal_network_without_normalization.eps\\', format=\\'eps\\')\\nplt.savefig(\\'results_normal_network_without_normalization.png\\', dpi=250, bbox_inches=\"tight\")\\nplt.show()\\n\\n# Saving values as .csv file to plot figure 30\\nnp.savetxt(\"results_normal_network_deviation_without_normalization.csv\", np.column_stack((testLabels, y_m3)), delimiter=\",\", header=\"original, predicted\")\\n\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss Plot\n",
    "\"\"\"\n",
    "import matplotlib\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "plt.rcParams.update({'font.size': 25})\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6), dpi=250)\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig('loss_normal_network_without_normalization.eps', format='eps')\n",
    "plt.savefig('loss_normal_network_without_normalization.png', dpi=250, bbox_inches=\"tight\")\n",
    "#plt.show()\n",
    "\n",
    "#Plotting results for the validation Data\n",
    "X_test = np.arange(len(Scans)-split)\n",
    "fig = plt.figure(figsize=(10, 6), dpi=250)\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.bar(X_test + 0.00, testLabels, color = 'tab:red', width = 0.15)\n",
    "ax.bar(X_test + 0.25, y_m3, color = 'green', width = 0.15)\n",
    "ax.set_ylabel('Vol. fiber percentage')\n",
    "ax.set_xlabel('testScans')\n",
    "ax.legend(labels=['testLabels', 'prediction'])\n",
    "plt.savefig('results_normal_network_without_normalization.eps', format='eps')\n",
    "plt.savefig('results_normal_network_without_normalization.png', dpi=250, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Saving values as .csv file to plot figure 30\n",
    "np.savetxt(\"results_normal_network_deviation_without_normalization.csv\", np.column_stack((testLabels, y_m3)), delimiter=\",\", header=\"original, predicted\")\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f17bf076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\ndef get_model_parametrized(depth, width, height, n_filters, n_layers, dropout, kernel, pool):\\n    inputs = keras.Input((depth, width, height, 1))\\n\\n    x = layers.Conv3D(filters=n_filters, kernel_size=kernel, activation=\"relu\")(inputs)\\n    x = layers.MaxPooling3D(pool_size=pool)(x)\\n    \\n    for layer in range(1, n_layers+1):\\n        x = layers.Dense(units=8*2**layer, activation=\"relu\")(x)\\n        x = layers.Dropout(dropout)(x)\\n    \\n    x = layers.GlobalAveragePooling3D()(x)\\n    \\n    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\\n    return keras.Model(inputs, outputs, name=\"3dcnn\")\\n\\n\\n# Defining which and how many veriables are used for the sweep\\n# For actual sweep executions, a smaller set was used to avoid having to do 1500 iterations in one run\\nfilters_list = [2, 4, 8, 16, 32]\\nlayers_list = [1, 2, 3, 4, 5]\\ndropout_list = [0, 0.1, 0.3, 0.5, 0.8]\\nkernel_list = [1, 3, 5, 7]\\npool_list = [2, 3, 4]\\n\\nn_Iterations = len(filters_list)*len(layers_list)*len(dropout_list)*len(kernel_list)*len(pool_list)\\nresults = np.zeros((n_Iterations, 8))\\n\\nsweepCounter = 0\\nfor n_filters in filters_list:\\n    for n_layers in layers_list:\\n        for dropout in dropout_list:\\n            for kernel in kernel_list:\\n                for pool in pool_list:\\n                    print(\"n_filters: \", n_filters, \"| \", \"n_layers:  \", n_layers, \"| \", \"dropout:  \", dropout, \"| \", \"kernel:  \", kernel, \"| \", \"pool:  \", pool)\\n                    print(\"Iteration: \", sweepCounter+1, \"/\", n_Iterations)\\n\\n                    # Training Setup\\n\\n                    initial_learning_rate = 0.0001\\n                    lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True) # Defining adjustable learning rate\\n    \\n                    model = get_model_parametrized(depth=newImageResolution, width=newImageResolution, height=newImageResolution, n_filters=n_filters, n_layers = n_layers, dropout=dropout, kernel=kernel, pool=pool)\\n                    model.compile(\\n                        loss=\"binary_crossentropy\"\\n                        ,optimizer=keras.optimizers.Adam(learning_rate=lr_schedule)\\n                        ,metrics=[\\'mae\\']\\n                    )\\n\\n                    # Initial Prediction (without training)\\n                    y_m1 = model.predict(InputScans)\\n\\n                    # Train Model\\n                    epochs = 40\\n                    hist = model.fit(Scans, FibrePerc, epochs = epochs, validation_split = 1 - split_border, verbose=0)\\n\\n                    # New Prediction (after training)\\n                    y_m2 = model.predict(InputScans)\\n                    \\n                    # Predicting validation data\\n                    y_m3 = model.predict(TestScans)\\n                    print(\"Result from test data prediction:\")\\n                    deviation(testLabels, y_m3)\\n\\n                    #Saving Data\\n                    results[sweepCounter][0] = getSAD()\\n                    results[sweepCounter][1] = getSAD_rel()\\n                    results[sweepCounter][2] = n_filters\\n                    results[sweepCounter][3] = n_layers\\n                    results[sweepCounter][4] = dropout\\n                    results[sweepCounter][5] = kernel\\n                    results[sweepCounter][6] = pool\\n                    sweepCounter += 1\\n\\nnp.savetxt(\"results.csv\", results, delimiter=\",\", header=\"SAD, SAD relative, n_filters, n_layers, dropout, n_kernels, pool\")\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional parameter sweep to optimize the network\n",
    "\n",
    "\"\"\" \n",
    "def get_model_parametrized(depth, width, height, n_filters, n_layers, dropout, kernel, pool):\n",
    "    inputs = keras.Input((depth, width, height, 1))\n",
    "\n",
    "    x = tf.keras.layers.Normalization(axis=-1, invert=True)\n",
    "\n",
    "    x = layers.Conv3D(filters=n_filters, kernel_size=kernel, activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPooling3D(pool_size=pool)(x)\n",
    "    \n",
    "    for layer in range(1, n_layers+1):\n",
    "        x = layers.Dense(units=8*2**layer, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling3D()(x)\n",
    "    \n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
    "    return keras.Model(inputs, outputs, name=\"3dcnn\")\n",
    "\n",
    "\n",
    "# Defining which and how many veriables are used for the sweep\n",
    "# For actual sweep executions, a smaller set was used to avoid having to do 1500 iterations in one run\n",
    "filters_list = [2, 4, 8, 16, 32]\n",
    "layers_list = [1, 2, 3, 4, 5]\n",
    "dropout_list = [0, 0.1, 0.3, 0.5, 0.8]\n",
    "kernel_list = [1, 3, 5, 7]\n",
    "pool_list = [2, 3, 4]\n",
    "\n",
    "n_Iterations = len(filters_list)*len(layers_list)*len(dropout_list)*len(kernel_list)*len(pool_list)\n",
    "results = np.zeros((n_Iterations, 8))\n",
    "\n",
    "sweepCounter = 0\n",
    "for n_filters in filters_list:\n",
    "    for n_layers in layers_list:\n",
    "        for dropout in dropout_list:\n",
    "            for kernel in kernel_list:\n",
    "                for pool in pool_list:\n",
    "                    print(\"n_filters: \", n_filters, \"| \", \"n_layers:  \", n_layers, \"| \", \"dropout:  \", dropout, \"| \", \"kernel:  \", kernel, \"| \", \"pool:  \", pool)\n",
    "                    print(\"Iteration: \", sweepCounter+1, \"/\", n_Iterations)\n",
    "\n",
    "                    # Training Setup\n",
    "\n",
    "                    initial_learning_rate = 0.0001\n",
    "                    lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True) # Defining adjustable learning rate\n",
    "    \n",
    "                    model = get_model_parametrized(depth=newImageResolution, width=newImageResolution, height=newImageResolution, n_filters=n_filters, n_layers = n_layers, dropout=dropout, kernel=kernel, pool=pool)\n",
    "                    model.compile(\n",
    "                        loss=\"binary_crossentropy\"\n",
    "                        ,optimizer=keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "                        ,metrics=['mae']\n",
    "                    )\n",
    "\n",
    "                    # Initial Prediction (without training)\n",
    "                    y_m1 = model.predict(InputScans)\n",
    "\n",
    "                    # Train Model\n",
    "                    epochs = 40\n",
    "                    hist = model.fit(Scans, FibrePerc, epochs = epochs, validation_split = 1 - split_border, verbose=0)\n",
    "\n",
    "                    # New Prediction (after training)\n",
    "                    y_m2 = model.predict(InputScans)\n",
    "                    \n",
    "                    # Predicting validation data\n",
    "                    y_m3 = model.predict(TestScans)\n",
    "                    print(\"Result from test data prediction:\")\n",
    "                    deviation(testLabels, y_m3)\n",
    "\n",
    "                    #Saving Data\n",
    "                    results[sweepCounter][0] = getSAD()\n",
    "                    results[sweepCounter][1] = getSAD_rel()\n",
    "                    results[sweepCounter][2] = n_filters\n",
    "                    results[sweepCounter][3] = n_layers\n",
    "                    results[sweepCounter][4] = dropout\n",
    "                    results[sweepCounter][5] = kernel\n",
    "                    results[sweepCounter][6] = pool\n",
    "                    sweepCounter += 1\n",
    "\n",
    "np.savetxt(\"results.csv\", results, delimiter=\",\", header=\"SAD, SAD relative, n_filters, n_layers, dropout, n_kernels, pool\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowTest",
   "language": "python",
   "name": "tensorflowtest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
